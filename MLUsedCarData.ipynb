{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sean Dube\n",
    "ID 5059 Coursework 1 \n",
    "\n",
    "Step 1. Frame the Problem\n",
    "\n",
    "The task is to predict list price of US Used Cars from a subset of attributes. The data used is from the US Used Cars dataset on Kaggle, posted by Ananay Mital. This analysis will explore the structure of the data to identify a small number of attributes that could plausibly correlate well with price. Using ML methods, we will attempt to answer this question. Our decision on what method to use will based on derived models and predictions. The performance measure we will use to analyze these models is RMSE or Root Mean Square Error\n",
    "\n",
    "First, we load the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel Python 3.10.5 is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Import Data and take a glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars = pandas.read_csv(\"/Users/seandube/Desktop/ST ANDREWS 2022-2023/ Courses Semester 2/ID5059/Coursework /used_cars_data_large_0.csv\", dtype={\"bed\": \"string\", \"dealer_zip\": \"string\"})\n",
    "cars.head(6) \n",
    "# Clear the maximum number of columns to be displayed, so that all will be visible.\n",
    "pandas.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Explore the Data\n",
    "\n",
    "Look at histograms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.hist(bins=50, figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Data and Check Split and then make a copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(cars, test_size=0.2, random_state=314)\n",
    "\n",
    "# check split \n",
    "\n",
    "len(test_set) / len (cars) # we get 0.2 so that works\n",
    "\n",
    "cars = train_set.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we explore the data for real. Take a look at the spatial plot, correlation matrix for each attribute, and attribute correlations with price. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carsplot = cars.plot(kind = \"scatter\", x = \"longitude\", y = \"latitude\", figsize =(10, 7))\n",
    "carsplot.set_aspect(\"equal\")\n",
    "plt.show()\n",
    "\n",
    "#correlation matrix for all attributes\n",
    "cormat = cars.corr()\n",
    "sn.heatmap(cormat)\n",
    "plt.show()\n",
    "\n",
    "#check correlations with price\n",
    "cormat[\"price\"].sort_values(ascending=True) #check how attributes are correlated to price\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mileage, Highway and City Fuel Economy, Owner Count, Horsepower, Year, and Engine Displacement appear to have the highest correlations with Price.\n",
    "\n",
    "Step 4. Prepare the data\n",
    "\n",
    "To begin we seperate the labels and drop price from the training set. Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_labels = cars[\"price\"].copy()\n",
    "cars_labels.head()\n",
    "\n",
    "cars = cars.drop(columns=\"price\")\n",
    "\n",
    "cars.info() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are bunch of attributes with missing values, so for convenience, I will only include columns relevant based on correlations to the price attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newcars = cars.filter(items = [\"mileage\", \"highway_fuel_economy\", \"owner_count\", \"city_fuel_economy\",\n",
    "  \"engine_displacement\", \"year\", \"horsepower\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to sort out missing values within our new training set. To do this, I use an imputer and impute by the median. Here, I create a pipeline, and transformer, so we don't have to do this manually in \n",
    "future steps. \n",
    "\n",
    "Apply the pipeline to our new training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "numerical_attributes = [\"mileage\", \"highway_fuel_economy\", \"owner_count\", \"city_fuel_economy\",\n",
    "                               \"engine_displacement\", \"year\", \"horsepower\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"numerical\", numerical_pipeline, numerical_attributes),\n",
    "])\n",
    "\n",
    "cars_prepared = full_pipeline.fit_transform(newcars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a glance at the new set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame(cars_prepared).info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears all the columns have the same number of rows and no null values. Success!\n",
    "\n",
    "Step 5. Explore the models. \n",
    "\n",
    "Here, I will test multiple models; Linear Regression, Decision Tree, Cross Validation, Random Forest, and Random Forest with Cross Validation!\n",
    "\n",
    "Let's start with Linear Regression. First, I will fit a linear regression model, the generate predictions for some test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(cars_prepared, cars_labels)\n",
    "\n",
    "some_data = test_set.iloc[:5]\n",
    "pandas.DataFrame(some_data)\n",
    "\n",
    "\n",
    "some_labels = cars_labels.iloc[:5]\n",
    "pandas.DataFrame(some_labels)\n",
    "\n",
    "some_data_prepared = full_pipeline.transform(some_data)\n",
    "pandas.DataFrame(some_data_prepared)\n",
    "\n",
    "#make predictions\n",
    "some_predictions = linear_regression.predict(some_data_prepared).round()\n",
    "some_predictions\n",
    "some_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row shows that the predicted price is about 20,655 whereas the actual value is 6,900.\n",
    "\n",
    "Next, we calculate RMSE (Root Mean Square Error) which is our model performance measure. We calculate this for our predictions over the big test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "linear_regression_cars_predictions = linear_regression.predict(cars_prepared)\n",
    "linear_regression_mse = mean_squared_error(cars_labels, linear_regression_cars_predictions)\n",
    "linear_regression_rmse = numpy.sqrt(linear_regression_mse)\n",
    "numpy.round(linear_regression_rmse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE = 16089.0 \n",
    "\n",
    "This does not look very accurate: Error is high in proportion to price. \n",
    "\n",
    "It's time to try another model. We will try the decision tree model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_regressor = DecisionTreeRegressor(random_state=42)\n",
    "tree_regressor.fit(cars_prepared, cars_labels)\n",
    "\n",
    "tree_regressor_cars_predictions = tree_regressor.predict(cars_prepared)\n",
    "tree_regressor_mse = mean_squared_error(cars_labels, tree_regressor_cars_predictions)\n",
    "tree_regressor_rmse = numpy.sqrt(tree_regressor_mse)\n",
    "tree_regressor_rmse"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE = 4429.864...\n",
    "This is much lower compared to the linear regression RMSE. \n",
    "\n",
    "Let's try Cross Validation and get the K Scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "K = 10\n",
    "\n",
    "tree_regressor_scores = cross_val_score(tree_regressor, cars_prepared, cars_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=K)\n",
    "tree_regressor_rmse_scores = numpy.sqrt(-tree_regressor_scores)\n",
    "\n",
    "# Retreive K scores\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", numpy.round(scores))\n",
    "    print(\"Mean:\", numpy.round(scores.mean()))\n",
    "    print(\"Standard deviation:\", numpy.round(scores.std()))\n",
    "\n",
    "display_scores(tree_regressor_rmse_scores)\n",
    "numpy.mean(tree_regressor_rmse_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean of the K Scores for K=10 folds is 13441.528273423377. We will need to compare this with the linear regression scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_scores = cross_val_score(linear_regression, cars_prepared, cars_labels,\n",
    "                                           scoring=\"neg_mean_squared_error\", cv=K)\n",
    "linear_regression_rmse_scores = numpy.sqrt(-linear_regression_scores)\n",
    "display_scores(linear_regression_rmse_scores)\n",
    "\n",
    "numpy.mean(linear_regression_rmse_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean of linear regression scores = 15586.473\n",
    "\n",
    "Cross Validation Scores are better compared to the linear regressions scores. Cross Validation could be a move.\n",
    "\n",
    "Lets try with K = 5 and see if there is any difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "K = 5\n",
    "\n",
    "tree_regressor_scores = cross_val_score(tree_regressor, cars_prepared, cars_labels,\n",
    "                         scoring=\"neg_mean_squared_error\", cv=K)\n",
    "tree_regressor_rmse_scores = numpy.sqrt(-tree_regressor_scores)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", numpy.round(scores))\n",
    "    print(\"Mean:\", numpy.round(scores.mean()))\n",
    "    print(\"Standard deviation:\", numpy.round(scores.std()))\n",
    "\n",
    "display_scores(tree_regressor_rmse_scores)\n",
    "\n",
    "numpy.mean(tree_regressor_rmse_scores) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean of K Scores for K=5 is 13441.528\n",
    "\n",
    "There is not much improvement from the K = 10. Let's try the Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "forest_regressor.fit(cars_prepared, cars_labels)\n",
    "\n",
    "forest_regressor_cars_predictions = forest_regressor.predict(cars_prepared)\n",
    "forest_regressor_mse = mean_squared_error(cars_labels, forest_regressor_cars_predictions)\n",
    "forest_regressor_rmse = numpy.sqrt(forest_regressor_mse)\n",
    "forest_regressor_rmse.round()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE = 6224.0, but also lets try it with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_regressor_scores = cross_val_score(forest_regressor, cars_prepared, cars_labels, scoring=\"neg_mean_squared_error\", cv=K)\n",
    "forest_regressor_rmse_scores = numpy.sqrt(-forest_regressor_scores)\n",
    "display_scores(forest_regressor_rmse_scores) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean RMSE scores is 11695 which is the best performer yet!\n",
    "\n",
    "Step 6. Fine Tune Models\n",
    "\n",
    "We found that Random Forest with Cross Validation was the best performing model, so we continue with this and try to improve it by changing hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameter_grid = [\n",
    "    # Try 12 (3×4) combinations of hyperparameters:\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "\n",
    "    # Then try 6 (2×3) combinations with bootstrap set as False:\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "\n",
    "forest_regressor = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Train across 5 folds, giving a total of (12+6)*5=90 rounds of training.\n",
    "grid_search = GridSearchCV(forest_regressor, parameter_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(cars_prepared, cars_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the best hyperparameter and calculate RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_\n",
    "\n",
    "grid_search_results = grid_search.cv_results_\n",
    "for mean_score, params in zip(grid_search_results[\"mean_test_score\"], grid_search_results[\"params\"]):\n",
    "    print(numpy.round(numpy.sqrt(-mean_score)), params)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best estimator was with 2 features and 30 estimators \n",
    "\n",
    "Now we set our Final Model, run it on the test set, and calculate some final predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "X_test = test_set.drop(\"price\", axis=1)\n",
    "y_test = test_set[\"price\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = numpy.sqrt(final_mse)\n",
    "final_rmse.round()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final RMSE = 10,114.0 \n",
    "Best performing model was Random Forest with Cross Validation with K = 5 and 30 estimators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
